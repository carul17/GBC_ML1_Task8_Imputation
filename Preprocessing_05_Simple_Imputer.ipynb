{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleImputer\n",
    "### This notebook outlines the usage of Simple Imputer (Univariate Imputation).\n",
    "### Simple Imputer substitutes missing values statistics (mean, median, ...)\n",
    "#### Dataset: [https://github.com/subashgandyer/datasets/blob/main/heart_disease.csv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demographic**\n",
    "- Sex: male or female(Nominal)\n",
    "- Age: Age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\n",
    "\n",
    "**Behavioral**\n",
    "- Current Smoker: whether or not the patient is a current smoker (Nominal)\n",
    "- Cigs Per Day: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarettes, even half a cigarette.)\n",
    "\n",
    "**Medical(history)**\n",
    "- BP Meds: whether or not the patient was on blood pressure medication (Nominal)\n",
    "- Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)\n",
    "- Prevalent Hyp: whether or not the patient was hypertensive (Nominal)\n",
    "- Diabetes: whether or not the patient had diabetes (Nominal)\n",
    "\n",
    "**Medical(current)**\n",
    "- Tot Chol: total cholesterol level (Continuous)\n",
    "- Sys BP: systolic blood pressure (Continuous)\n",
    "- Dia BP: diastolic blood pressure (Continuous)\n",
    "- BMI: Body Mass Index (Continuous)\n",
    "- Heart Rate: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.)\n",
    "- Glucose: glucose level (Continuous)\n",
    "\n",
    "**Predict variable (desired target)**\n",
    "- 10 year risk of coronary heart disease CHD (binary: “1”, means “Yes”, “0” means “No”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"heart_disease.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many Categorical variables in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many Missing values in the dataset?\n",
    "Hint: df.Series.isna( ).sum( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df.columns)):\n",
    "    missing_data = df[df.columns[i]].isna().sum()\n",
    "    perc = missing_data / len(df) * 100\n",
    "    print(f'Feature {i+1} >> Missing entries: {missing_data}  |  Percentage: {round(perc, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Visual representation of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df.isna(), cbar=False, cmap='viridis', yticklabels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SimpleImputer object with 'mean' strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional - converting df into numpy array (There is a way to directly impute from dataframe as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, :-1]\n",
    "y = data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the imputer model on dataset to calculate statistic for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained imputer model is applied to dataset to create a copy of dataset with all filled missing values from the calculated statistic using transform( ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform = imputer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check: Whether missing values are filled or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in original data\n",
    "print(\"Missing values before imputation:\")\n",
    "print(f\"Total missing values: {df.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in transformed data\n",
    "print(\"Missing values after imputation:\")\n",
    "print(f\"Total missing values: {df_transform.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to visualize the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df.isna(), cbar=False, cmap='viridis', yticklabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(X_transform.isna(), cbar=False, cmap='viridis', yticklabels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the issue here?\n",
    "#### Hint: Heatmap needs a DataFrame and not a Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert transformed array back to DataFrame for visualization\n",
    "df_transform = pd.DataFrame(X_transform, columns=df.columns[:-1])\n",
    "df_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df_transform.isna(), cbar=False, cmap='viridis', yticklabels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if these datasets contain missing data\n",
    "### Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"X_train.csv\")\n",
    "Y_train = pd.read_csv(\"Y_train.csv\")\n",
    "Y_test = pd.read_csv(\"Y_test.csv\")\n",
    "X_test = pd.read_csv(\"X_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(X_train.isna(), cbar=False, cmap='viridis', yticklabels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there missing data in this dataset???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in all datasets\n",
    "print(\"Missing values in X_train:\", X_train.isna().sum().sum())\n",
    "print(\"Missing values in Y_train:\", Y_train.isna().sum().sum())\n",
    "print(\"Missing values in X_test:\", X_test.isna().sum().sum())\n",
    "print(\"Missing values in Y_test:\", Y_test.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Logistic Regression model Without imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"heart_disease.csv\")\n",
    "X = df[df.columns[:-1]]\n",
    "y = df[df.columns[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop all rows with missing entries - Build a Logistic Regression model and benchmark the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"heart_disease.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "df_clean = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_clean[df_clean.columns[:-1]]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_clean[df_clean.columns[-1]]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pipeline with model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('model', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a RepeatedStratifiedKFold with 10 splits and 3 repeats and random_state=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-validation object\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call cross_val_score with pipeline, X, y, accuracy metric and cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model using cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the Mean Accuracy and Standard Deviation from scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Accuracy: {round(np.mean(scores), 3)}  | Std: {round(np.std(scores), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Logistic Regression model with SimpleImputer Mean Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"heart_disease.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df.columns[:-1]]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[df.columns[-1]]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SimpleImputer with mean strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SimpleImputer with mean strategy\n",
    "imputer = SimpleImputer(strategy='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LogisticRegression model\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pipeline with impute and model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with imputer and model\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', imputer),\n",
    "    ('model', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a RepeatedStratifiedKFold with 10 splits and 3 repeats and random_state=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-validation object\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call cross_val_score with pipeline, X, y, accuracy metric and cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model using cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the Mean Accuracy and Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Accuracy: {round(np.mean(scores), 3)}  | Std: {round(np.std(scores), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which accuracy is better? \n",
    "- Dropping missing values\n",
    "- SimpleImputer with Mean Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparison of Strategies:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"1. Dropping missing values:\")\n",
    "print(f\"Mean Accuracy: {round(0.848, 3)}  | Std: {round(0.036, 3)}\")\n",
    "print(\"\\n2. SimpleImputer with Mean Strategy:\")\n",
    "print(f\"Mean Accuracy: {round(0.851, 3)}  | Std: {round(0.035, 3)}\")\n",
    "print(\"\\nConclusion: SimpleImputer with Mean Strategy performs slightly better\")\n",
    "print(\"and preserves more data compared to dropping rows with missing values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimpleImputer Mean - Benchmark after Mean imputation with RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SimpleImputer with mean strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SimpleImputer with mean strategy\n",
    "imputer = SimpleImputer(strategy='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a RandomForest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RandomForest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with imputer and RandomForest\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', imputer),\n",
    "    ('model', rf_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-validation object\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model using cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Mean Accuracy and Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(f\"Mean Accuracy: {round(np.mean(scores), 3)}  | Std: {round(np.std(scores), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "# Run experiments with different Strategies and different algorithms\n",
    "\n",
    "## STRATEGIES\n",
    "- Mean\n",
    "- Median\n",
    "- Most_frequent\n",
    "- Constant\n",
    "\n",
    "## ALGORITHMS\n",
    "- Logistic Regression\n",
    "- KNN\n",
    "- Random Forest\n",
    "- SVM\n",
    "- Any other algorithm of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint: Collect the pipeline creation, KFold, and Cross_Val_Score inside a for loop and iterate over different strategies in a list and different algorithms in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of all combinations:\n",
      "--------------------------------------------------------------------------------\n",
      "Strategy: mean            Model: Logistic Regression  Accuracy: 0.855 ± 0.005\n",
      "Strategy: mean            Model: KNN                  Accuracy: 0.837 ± 0.009\n",
      "Strategy: mean            Model: Random Forest        Accuracy: 0.850 ± 0.006\n",
      "Strategy: mean            Model: SVM                  Accuracy: 0.848 ± 0.002\n",
      "Strategy: mean            Model: Gradient Boosting    Accuracy: 0.846 ± 0.008\n",
      "Strategy: median          Model: Logistic Regression  Accuracy: 0.855 ± 0.006\n",
      "Strategy: median          Model: KNN                  Accuracy: 0.836 ± 0.008\n",
      "Strategy: median          Model: Random Forest        Accuracy: 0.850 ± 0.006\n",
      "Strategy: median          Model: SVM                  Accuracy: 0.848 ± 0.002\n",
      "Strategy: median          Model: Gradient Boosting    Accuracy: 0.846 ± 0.009\n",
      "Strategy: most_frequent   Model: Logistic Regression  Accuracy: 0.855 ± 0.006\n",
      "Strategy: most_frequent   Model: KNN                  Accuracy: 0.835 ± 0.009\n",
      "Strategy: most_frequent   Model: Random Forest        Accuracy: 0.849 ± 0.006\n",
      "Strategy: most_frequent   Model: SVM                  Accuracy: 0.848 ± 0.002\n",
      "Strategy: most_frequent   Model: Gradient Boosting    Accuracy: 0.846 ± 0.007\n",
      "Strategy: constant        Model: Logistic Regression  Accuracy: 0.854 ± 0.005\n",
      "Strategy: constant        Model: KNN                  Accuracy: 0.837 ± 0.010\n",
      "Strategy: constant        Model: Random Forest        Accuracy: 0.848 ± 0.006\n",
      "Strategy: constant        Model: SVM                  Accuracy: 0.848 ± 0.002\n",
      "Strategy: constant        Model: Gradient Boosting    Accuracy: 0.846 ± 0.008\n"
     ]
    }
   ],
   "source": [
    "# Import required models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Define strategies and models to test\n",
    "strategies = ['mean', 'median', 'most_frequent', 'constant']\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=10000)), # Increase max_iter to avoid convergence warnings\n",
    "    ('KNN', KNeighborsClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=100, random_state=1)),\n",
    "    ('SVM', SVC(random_state=1)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(n_estimators=100, random_state=1))\n",
    "]\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Create cross-validation object\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Iterate through all combinations\n",
    "for strategy in strategies:\n",
    "    for model_name, model in models:\n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy=strategy)),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        # Evaluate model\n",
    "        scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'strategy': strategy,\n",
    "            'model': model_name,\n",
    "            'mean_accuracy': np.mean(scores),\n",
    "            'std': np.std(scores)\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display results in a formatted table\n",
    "print(\"Results of all combinations:\")\n",
    "print(\"-\" * 80)\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"Strategy: {row['strategy']:<15} Model: {row['model']:<20} \"\n",
    "          f\"Accuracy: {row['mean_accuracy']:.3f} ± {row['std']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Which is the best strategy for this dataset using Random Forest algorithm?\n",
    "- MEAN\n",
    "- MEDIAN\n",
    "- MOST_FREQUENT\n",
    "- CONSTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest performance with different strategies:\n",
      "--------------------------------------------------\n",
      "Strategy: mean            Accuracy: 0.850 ± 0.006\n",
      "Strategy: median          Accuracy: 0.850 ± 0.006\n",
      "Strategy: most_frequent   Accuracy: 0.849 ± 0.006\n",
      "Strategy: constant        Accuracy: 0.848 ± 0.006\n",
      "\n",
      "Best strategy for Random Forest: mean with accuracy 0.850 ± 0.006\n"
     ]
    }
   ],
   "source": [
    "# Filter results for Random Forest\n",
    "rf_results = results_df[results_df['model'] == 'Random Forest']\n",
    "print(\"Random Forest performance with different strategies:\")\n",
    "print(\"-\" * 50)\n",
    "for _, row in rf_results.iterrows():\n",
    "    print(f\"Strategy: {row['strategy']:<15} Accuracy: {row['mean_accuracy']:.3f} ± {row['std']:.3f}\")\n",
    "    \n",
    "best_rf = rf_results.loc[rf_results['mean_accuracy'].idxmax()]\n",
    "print(f\"\\nBest strategy for Random Forest: {best_rf['strategy']} with accuracy {best_rf['mean_accuracy']:.3f} ± {best_rf['std']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2:  Which is the best algorithm for this dataset using Mean Strategy?\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- KNN\n",
    "- any other algorithm of your choice (BONUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean strategy performance with different algorithms:\n",
      "--------------------------------------------------\n",
      "Algorithm: Logistic Regression  Accuracy: 0.855 ± 0.005\n",
      "Algorithm: KNN                  Accuracy: 0.837 ± 0.009\n",
      "Algorithm: Random Forest        Accuracy: 0.850 ± 0.006\n",
      "Algorithm: SVM                  Accuracy: 0.848 ± 0.002\n",
      "Algorithm: Gradient Boosting    Accuracy: 0.846 ± 0.008\n",
      "\n",
      "Best algorithm with Mean strategy: Logistic Regression with accuracy 0.855 ± 0.005\n"
     ]
    }
   ],
   "source": [
    "# Filter results for Mean strategy\n",
    "mean_results = results_df[results_df['strategy'] == 'mean']\n",
    "print(\"Mean strategy performance with different algorithms:\")\n",
    "print(\"-\" * 50)\n",
    "for _, row in mean_results.iterrows():\n",
    "    print(f\"Algorithm: {row['model']:<20} Accuracy: {row['mean_accuracy']:.3f} ± {row['std']:.3f}\")\n",
    "    \n",
    "best_mean = mean_results.loc[mean_results['mean_accuracy'].idxmax()]\n",
    "print(f\"\\nBest algorithm with Mean strategy: {best_mean['model']} with accuracy {best_mean['mean_accuracy']:.3f} ± {best_mean['std']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: Which is the best combination of algorithm and best Imputation Strategy overall?\n",
    "- Mean , Median, Most_frequent, Constant\n",
    "- Logistic Regression, Random Forest, KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best overall combination:\n",
      "--------------------------------------------------\n",
      "Strategy: most_frequent\n",
      "Algorithm: Logistic Regression\n",
      "Accuracy: 0.855 ± 0.006\n"
     ]
    }
   ],
   "source": [
    "# Find best overall combination\n",
    "best_overall = results_df.loc[results_df['mean_accuracy'].idxmax()]\n",
    "print(\"Best overall combination:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Strategy: {best_overall['strategy']}\")\n",
    "print(f\"Algorithm: {best_overall['model']}\")\n",
    "print(f\"Accuracy: {best_overall['mean_accuracy']:.3f} ± {best_overall['std']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Summary\n",
    "\n",
    "After comparing different imputation strategies and machine learning algorithms, we can draw several conclusions:\n",
    "\n",
    "1. **Best Imputation Strategy for Random Forest:**\n",
    "   - Mean and Median strategies tied with accuracy of 0.850 ± 0.006\n",
    "   - Most_frequent (0.849 ± 0.006) and Constant (0.848 ± 0.006) performed slightly worse\n",
    "   - The small difference between strategies suggests Random Forest is robust to the choice of imputation method\n",
    "\n",
    "2. **Best Algorithm with Mean Strategy:**\n",
    "   - Logistic Regression performed best (0.855 ± 0.005)\n",
    "   - Random Forest was second (0.850 ± 0.006)\n",
    "   - KNN performed worst (0.837 ± 0.009)\n",
    "   - SVM (0.848 ± 0.002) showed the lowest standard deviation\n",
    "\n",
    "3. **Best Overall Combination:**\n",
    "   - Most_frequent imputation + Logistic Regression (0.855 ± 0.006)\n",
    "   - Mean imputation + Logistic Regression performed equally well (0.855 ± 0.005)\n",
    "   - The small performance differences suggest the dataset is well-behaved\n",
    "\n",
    "4. **Key Takeaways:**\n",
    "   - Simple imputation strategies work well for this dataset\n",
    "   - Logistic Regression consistently outperformed other algorithms\n",
    "   - The choice of imputation strategy has minimal impact on performance\n",
    "   - Low standard deviations across all methods indicate stable and reliable predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
